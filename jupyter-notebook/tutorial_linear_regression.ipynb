{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hD9kXDqe1fX7"
      },
      "source": [
        "# Linear Regression Tutorial\n",
        "These codes were implemented by Emirhan Ozkan using the tutorial by Marc Deisenroth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdnjokud1fX8"
      },
      "source": [
        "The purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9zd-81bi1fX8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.linalg\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g8oO9EW1fX9"
      },
      "source": [
        "We consider a linear regression problem of the form\n",
        "$$\n",
        "y = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n",
        "$$\n",
        "where $\\boldsymbol x\\in\\mathbb{R}^D$ are inputs and $y\\in\\mathbb{R}$ are noisy observations. The parameter vector $\\boldsymbol\\theta\\in\\mathbb{R}^D$ parametrizes the function.\n",
        "\n",
        "We assume we have a training set $(\\boldsymbol x_n, y_n)$, $n=1,\\ldots, N$. We summarize the sets of training inputs in $\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}$ and corresponding training targets $\\mathcal Y = \\{y_1, \\ldots, y_N\\}$, respectively.\n",
        "\n",
        "In this tutorial, we are interested in finding good parameters $\\boldsymbol\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "fae0qHFZ1fX9",
        "outputId": "cf497dcc-a27c-4c43-fddd-5364b8499f15"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Define training set\n",
        "X = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n",
        "y = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n",
        "\n",
        "# Plot the training set\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+', markersize=10)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TlBS7eo1fX9"
      },
      "source": [
        "## 1. Maximum Likelihood\n",
        "We will start with maximum likelihood estimation of the parameters $\\boldsymbol\\theta$. In maximum likelihood estimation, we find the parameters $\\boldsymbol\\theta^{\\mathrm{ML}}$ that maximize the likelihood\n",
        "$$\n",
        "p(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n",
        "$$\n",
        "From the lecture we know that the maximum likelihood estimator is given by\n",
        "$$\n",
        "\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n",
        "$$\n",
        "where \n",
        "$$\n",
        "\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FG6zwDW1fX9"
      },
      "source": [
        "Let us compute the maximum likelihood estimate for a given training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6vbPOfMv1fX-"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDITED FUNCTION\n",
        "def max_lik_estimate(X, y):\n",
        "    \n",
        "    # X: N x D matrix of training inputs\n",
        "    # y: N x 1 vector of training targets/observations\n",
        "    # returns: maximum likelihood parameters (D x 1)\n",
        "    \n",
        "    N, D = X.shape\n",
        "    # theta_ml = np.zeros((D,1)) ## <-- EDIT THIS LINE\n",
        "    theta_ml = np.linalg.inv(X.T @ X) @ X.T @ y ## <-- EDITED LINE\n",
        "    return theta_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX1qgVk32TNJ"
      },
      "source": [
        "# **Explanation:**\n",
        "\n",
        "* The function takes in the inputs X (an N x D matrix of training inputs) and y (an N x 1 vector of training targets/observations).\n",
        "* It first extracts the dimensions of X and initializes theta_ml (the maximum likelihood parameters) to a D x 1 zero vector.\n",
        "* The maximum likelihood estimate for linear regression is given by the formula theta_ml = (X^T X)^-1 X^T y, where X^T is the transpose of X and ^-1 denotes matrix inversion. This formula can be derived by maximizing the likelihood function of the linear regression model with respect to theta.\n",
        "* The edited function uses this formula to compute theta_ml, and returns it as the output of the function.\n",
        "\n",
        "\n",
        "# **Edited Code Explanation:**\n",
        "\n",
        "np.linalg.inv is a NumPy function that computes the inverse of a square matrix.\n",
        "\n",
        "The syntax for using np.linalg.inv is:\n",
        "np.linalg.inv(a)\n",
        "\n",
        "where a is the input matrix.\n",
        "\n",
        "The function returns the inverse of a, which is a new matrix that when multiplied by a gives the identity matrix. In other words, if a_inv is the inverse of a, then a_inv @ a (matrix multiplication) will give the identity matrix.\n",
        "\n",
        "In the context of linear regression, np.linalg.inv(X.T @ X) is used to invert the matrix X.T @ X, which appears in the formula for computing the maximum likelihood estimate of the model parameters. The resulting inverse matrix is then multiplied by X.T @ y to obtain the maximum likelihood parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SwnPhm4R1fX-",
        "outputId": "76dffe0a-4565-4fb6-c418-29bf94f7e3cc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# get maximum likelihood estimate\n",
        "theta_ml = max_lik_estimate(X,y)\n",
        "\n",
        "print(\"Theta from maximum likelihood: \", theta_ml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgP6e6Sw1fX-"
      },
      "source": [
        "Now, make a prediction using the maximum likelihood estimate that we just found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "svjVM1sM1fX-"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDITED FUNCTION\n",
        "def predict_with_estimate(Xtest, theta):\n",
        "    \n",
        "    # Xtest: K x D matrix of test inputs\n",
        "    # theta: D x 1 vector of parameters\n",
        "    # returns: prediction of f(Xtest); K x 1 vector\n",
        "    \n",
        "    # prediction = Xtest ## <-- EDIT THIS LINE\n",
        "    prediction = Xtest @ theta ## <-- EDITED LINE\n",
        "    \n",
        "    return prediction "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XALQR4QD4Wqu"
      },
      "source": [
        "# **Explanation:**\n",
        "\n",
        "*   The function takes in the inputs Xtest (K x D\n",
        "matrix of test inputs) and theta (D x 1 vector of parameters).\n",
        "*   The prediction of the linear regression model for a new set of inputs Xtest is given by the formula f(Xtest) = Xtest @ theta, where @ denotes matrix multiplication.\n",
        "*   The edited function uses this formula to compute the prediction, and returns it as the output of the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut45WuDx1fX-"
      },
      "source": [
        "Now, let's see whether we got something useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "tzrSRYbV1fX-",
        "outputId": "dbcd5696-c7cb-4b04-8319-176d42250006"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# define a test set\n",
        "Xtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n",
        "\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(Xtest, theta_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+', markersize=10)\n",
        "plt.plot(Xtest, ml_prediction)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-IbJTDLZ1fX_"
      },
      "source": [
        "#### Questions\n",
        "1. Does the solution above look reasonable?\n",
        "  *   Answer: Yes, the solution above looks reasonable. The maximum likelihood estimate for $\\theta$ is computed using the closed-form solution for linear regression, and the predicted function values at the test points using this estimate seem to fit the training data well.\n",
        "2. Play around with different values of $\\theta$. How do the corresponding functions change?\n",
        "  *   Answer: When we change the values of $\\theta$, the corresponding functions change as follows:\n",
        "    *   If $\\theta$ is increased, the predicted function values increase as well.\n",
        "    *   If $\\theta$ is decreased, the predicted function values decrease as well.\n",
        "    *   If $\\theta$ is negative, the predicted function values are reflected about the y-axis.\n",
        "\n",
        "\n",
        "3. Modify the training targets $\\mathcal Y$ and re-run your computation. What changes?\n",
        "\n",
        "  *   Answer: If we modify the training targets $\\mathcal Y$, the maximum likelihood estimate for $\\theta$ will change accordingly. Specifically, if we increase the training targets, the predicted function values at the test points will also increase, and vice versa if we decrease the training targets. If the training targets are scaled or shifted by a constant, the maximum likelihood estimate for $\\theta$ will remain the same, but the predicted function values will be scaled or shifted accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1UsnNQl1fX_"
      },
      "source": [
        "Let us now look at a different training set, where we add 2.0 to every $y$-value, and compute the maximum likelihood estimate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "ytfrl-7F1fX_",
        "outputId": "951001d6-3d4c-4519-bcc2-d389e11700aa"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "ynew = y + 2.0\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, ynew, '+', markersize=10)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "collapsed": true,
        "id": "TFc0mWZd1fX_",
        "outputId": "fda0456c-bcb9-4df6-927b-3f3120597970"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# get maximum likelihood estimate\n",
        "theta_ml = max_lik_estimate(X, ynew)\n",
        "print(theta_ml)\n",
        "\n",
        "# define a test set\n",
        "Xtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n",
        "\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(Xtest, theta_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, ynew, '+', markersize=10)\n",
        "plt.plot(Xtest, ml_prediction)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcQKrKRZ1fX_"
      },
      "source": [
        "#### Question:\n",
        "1. This maximum likelihood estimate doesn't look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\n",
        "2. How can we fix this problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVeyRyHI1fX_"
      },
      "source": [
        "Let us now define a linear regression model that is slightly more flexible:\n",
        "$$\n",
        "y = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n",
        "$$\n",
        "Here, we added an offset (bias) parameter $\\theta_0$ to our original model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1hJM1eA1fX_"
      },
      "source": [
        "#### Question:\n",
        "1. What is the effect of this bias parameter, i.e., what additional flexibility does it offer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rIglGCi1fX_"
      },
      "source": [
        "If we now define the inputs to be the augmented vector $\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}$, we can write the new linear regression model as \n",
        "$$\n",
        "y = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n",
        "\\theta_0\\\\\n",
        "\\boldsymbol\\theta_1\n",
        "\\end{bmatrix}\\,.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UfBWUA811fX_"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "N, D = X.shape\n",
        "X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\n",
        "theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PMtVIL01fYA"
      },
      "source": [
        "Let us now compute the maximum likelihood estimator for this setting.\n",
        "_Hint:_ If possible, re-use code that you have already written"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0I3Yg4qm1fYA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def max_lik_estimate_aug(X_aug, y):\n",
        "    \n",
        "    theta_aug_ml = max_lik_estimate(X_aug, y) ## <-- EDITED LINE\n",
        "    \n",
        "    return theta_aug_ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e4Tnjux01fYA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "theta_aug_ml = max_lik_estimate_aug(X_aug, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R3g75rW1fYA"
      },
      "source": [
        "Now, we can make predictions again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "mpY9kFsM1fYA",
        "outputId": "9004afd2-91ec-4314-f444-aa4246db2049"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# define a test set (we also need to augment the test inputs with ones)\n",
        "Xtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n",
        "\n",
        "# predict the function values at the test points using the maximum likelihood estimator\n",
        "ml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+', markersize=10)\n",
        "plt.plot(Xtest, ml_prediction)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWdrvVAT1fYA"
      },
      "source": [
        "It seems this has solved our problem! \n",
        "#### Question:\n",
        "1. Play around with the first parameter of $\\boldsymbol\\theta_{\\text{aug}}$ and see how the fit of the function changes.\n",
        "2. Play around with the second parameter of $\\boldsymbol\\theta_{\\text{aug}}$ and see how the fit of the function changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lTLiDdn1fYA"
      },
      "source": [
        "### Nonlinear Features\n",
        "So far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs $\\boldsymbol x$, as long as the parameters $\\boldsymbol\\theta$ appear linearly. This means, we can learn functions of the form\n",
        "$$\n",
        "f(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n",
        "$$\n",
        "where the features $\\phi_k(\\boldsymbol x)$ are (possibly nonlinear) transformations of the inputs $\\boldsymbol x$.\n",
        "\n",
        "Let us have a look at an example where the observations clearly do not lie on a straight line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "2zyxbep11fYA",
        "outputId": "3d41e6f5-60d2-46c3-8295-32ee0bcc8bac"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "y = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BO7evDS1fYA"
      },
      "source": [
        "#### Polynomial Regression\n",
        "One class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree $K$ as\n",
        "$$\n",
        "\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n",
        "\\boldsymbol\\phi(x)= \n",
        "\\begin{bmatrix}\n",
        "x^0\\\\\n",
        "x^1\\\\\n",
        "\\vdots\\\\\n",
        "x^K\n",
        "\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n",
        "$$\n",
        "Here, $\\boldsymbol\\phi(x)$ is a nonlinear feature transformation of the inputs $x\\in\\mathbb{R}$.\n",
        "\n",
        "Similar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs:\n",
        "$$\n",
        "\\boldsymbol\\Phi = \\begin{bmatrix}\n",
        "\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n",
        "\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiIWDM5N1fYA"
      },
      "source": [
        "Let us start by computing the feature matrix $\\boldsymbol \\Phi$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0ggE8sfR1fYA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "In the context of polynomial regression, the poly_features function computes \n",
        "the feature matrix Phi of size N x (K+1) given the inputs X of size N x 1 and \n",
        "the degree of the polynomial K.\n",
        "\n",
        "The k-th column of Phi contains the values of X raised to the power of k. The \n",
        "first column of Phi contains 1's as the 0-th power of X is 1. Thus, the poly_features \n",
        "function computes the polynomial basis functions for X.\n",
        "\n",
        "These basis functions are used to fit a polynomial function of degree K to the \n",
        "training data by minimizing the sum of squared errors between the predicted values \n",
        "and the training targets.\n",
        "\"\"\"\n",
        "\n",
        "## EDITED FUNCTION\n",
        "def poly_features(X, K):\n",
        "    \n",
        "    # X: inputs of size N x 1\n",
        "    # K: degree of the polynomial\n",
        "    # computes the feature matrix Phi (N x (K+1))\n",
        "    \n",
        "    X = X.flatten()\n",
        "    N = X.shape[0]\n",
        "    \n",
        "    #initialize Phi\n",
        "    Phi = np.zeros((N, K+1))\n",
        "    \n",
        "    # Compute the feature matrix in stages\n",
        "    # Phi = np.zeros((N, K+1)) ## <-- EDIT THIS LINE\n",
        "    for k in range(K+1): ## <-- EDITED LINE\n",
        "        Phi[:,k] = X**k ## <-- EDITED LINE\n",
        "    return Phi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V-UeCUU1fYB"
      },
      "source": [
        "With this feature matrix we get the maximum likelihood estimator as\n",
        "$$\n",
        "\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n",
        "$$\n",
        "For reasons of numerical stability, we often add a small diagonal \"jitter\" $\\kappa>0$ to $\\boldsymbol\\Phi^T\\boldsymbol\\Phi$ so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes\n",
        "$$\n",
        "\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7bGQHE-O1fYB"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDITED FUNCTION\n",
        "def nonlinear_features_maximum_likelihood(Phi, y):\n",
        "    # Phi: features matrix for training inputs. Size of N x D\n",
        "    # y: training targets. Size of N by 1\n",
        "    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n",
        "    \n",
        "    kappa = 1e-08 # 'jitter' term; good for numerical stability\n",
        "    \n",
        "    D = Phi.shape[1]  \n",
        "    \n",
        "    # theta_ml = np.zeros((D,1)) ## <-- EDIT THIS LINE\n",
        "\n",
        "    # maximum likelihood estimate ## <-- EDITED LINE\n",
        "    Pt = Phi.T @ y # Phi^T*y  ## <-- EDITED LINE\n",
        "    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I ## <-- EDITED LINE\n",
        "\n",
        "    # maximum likelihood estimate\n",
        "    C = scipy.linalg.cho_factor(PP) ## <-- EDITED LINE\n",
        "    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y    ## <-- EDITED LINE\n",
        "    \n",
        "    return theta_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTwpIcvt1fYB"
      },
      "source": [
        "Now we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let's see how this works.\n",
        "\n",
        "To make predictions at test inputs $\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}$, we need to compute the features (nonlinear transformations) $\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})$ of $\\boldsymbol X_{\\text{test}}$ to give us the predicted mean\n",
        "$$\n",
        "\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "2oyLoyQl1fYB",
        "outputId": "4595b7aa-8dda-4f9f-8d5c-699bdf64dd50"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "K = 5 # Define the degree of the polynomial we wish to fit\n",
        "Phi = poly_features(X, K) # N x (K+1) feature matrix\n",
        "\n",
        "theta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n",
        "\n",
        "# test inputs\n",
        "Xtest = np.linspace(-4,4,100).reshape(-1,1)\n",
        "\n",
        "# feature matrix for test inputs\n",
        "Phi_test = poly_features(Xtest, K)\n",
        "\n",
        "y_pred = Phi_test @ theta_ml # predicted y-values\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.plot(Xtest, y_pred)\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yutDEvo1fYB"
      },
      "source": [
        "Experiment with different polynomial degrees in the code above.\n",
        "#### Questions:\n",
        "1. What do you observe?\n",
        "2. What is a good fit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dkf4wvh1fYB"
      },
      "source": [
        "## Evaluating the Quality of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzJjTto1fYB"
      },
      "source": [
        "Let us have a look at a more interesting data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "collapsed": true,
        "id": "kyIkDLeR1fYB",
        "outputId": "00c08793-75f7-41e7-b163-1539449c8a50"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def f(x):   \n",
        "    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n",
        "\n",
        "X = np.linspace(-4,4,20).reshape(-1,1)\n",
        "y = f(X)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKSOFpQm1fYB"
      },
      "source": [
        "Now, let us use the work from above and fit polynomials to this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "xeJyWWhS1fYC",
        "outputId": "216fdf34-0396-40a1-f425-1ab272469bca"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "K = 2 # Define the degree of the polynomial we wish to fit\n",
        "\n",
        "Phi = poly_features(X, K) # N x (K+1) feature matrix\n",
        "\n",
        "theta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n",
        "\n",
        "# test inputs\n",
        "Xtest = np.linspace(-5,5,100).reshape(-1,1)\n",
        "ytest = f(Xtest) # ground-truth y-values\n",
        "\n",
        "# feature matrix for test inputs\n",
        "Phi_test = poly_features(Xtest, K)\n",
        "\n",
        "y_pred = Phi_test @ theta_ml # predicted y-values # <-- EDITED LINE\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.plot(Xtest, y_pred)\n",
        "plt.plot(Xtest, ytest)\n",
        "plt.legend([\"data\", \"prediction\", \"ground truth observations\"])\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQzi6Jk51fYC"
      },
      "source": [
        "#### Questions:\n",
        "1. Try out different degrees of polynomials. \n",
        "2. Based on visual inspection, what looks like the best fit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRuzFGjt1fYC"
      },
      "source": [
        "Let us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the $y$-values predicted by our polynomial and the ground-truth $y$-values. The RMSE is then defined as\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n",
        "$$\n",
        "Write a function that computes the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Qa1_0BsS1fYC"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS FUNCTION\n",
        "def RMSE(y, ypred):\n",
        "    rmse = np.sqrt(np.mean((y-ypred)**2)) ## <-- EDITED LINE\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HlaCuph1fYC"
      },
      "source": [
        "Now compute the RMSE for different degrees of the polynomial we want to fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "a1J7Bjfr1fYC",
        "outputId": "c7241788-36e1-46b3-832b-7656a0501475"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "K_max = 20\n",
        "rmse_train = np.zeros((K_max+1,))\n",
        "\n",
        "for k in range(K_max+1): # <-- EDITED\n",
        "    \n",
        "     \n",
        "    # feature matrix\n",
        "    Phi = poly_features(X, k)\n",
        "    \n",
        "    # maximum likelihood estimate\n",
        "    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n",
        "    \n",
        "    # predict y-values of training set\n",
        "    ypred_train = Phi @ theta_ml\n",
        "    \n",
        "    # RMSE on training set\n",
        "    rmse_train[k] = RMSE(y, ypred_train)\n",
        "        \n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rmse_train)\n",
        "plt.xlabel(\"degree of polynomial\")\n",
        "plt.ylabel(\"RMSE\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vezf8KuF1fYC"
      },
      "source": [
        "#### Question: \n",
        "1. What do you observe?\n",
        "2. What is the best polynomial fit according to this plot?\n",
        "3. Write some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "BFD0ya3X1fYC",
        "outputId": "ed4ecab3-5063-4592-9424-ecfef6c4ab68"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# WRITE THE PLOTTING CODE HERE\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "\n",
        "# feature matrix\n",
        "Phi = poly_features(X, 5)\n",
        "\n",
        "# maximum likelihood estimate\n",
        "theta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n",
        "\n",
        "# feature matrix for test inputs\n",
        "Phi_test = poly_features(Xtest, 5)\n",
        "\n",
        "ypred_test = Phi_test @ theta_ml ## <--- EDITED LINE (hint: you may require a few lines to do the computation)\n",
        "\n",
        "plt.plot(Xtest, ypred_test) \n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.legend([\"data\", \"maximum likelihood fit\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G21FAzpQ1fYC"
      },
      "source": [
        "The RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "fnmjcBO51fYC",
        "outputId": "781edf0b-202c-41ba-e96e-5ca60e35d6e3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "K_max = 20\n",
        "rmse_train = np.zeros((K_max+1,))\n",
        "rmse_test = np.zeros((K_max+1,))\n",
        "\n",
        "for k in range(K_max+1):\n",
        "    \n",
        "    # feature matrix\n",
        "    Phi = poly_features(X, k) ## <--- EDITED LINE\n",
        "    \n",
        "    # maximum likelihood estimate\n",
        "    theta_ml = nonlinear_features_maximum_likelihood(Phi, y) ## <--- EDITED LINE\n",
        "    \n",
        "    # predict y-values of training set\n",
        "    ypred_train = Phi @ theta_ml ## <--- EDITED LINE\n",
        "    \n",
        "    # RMSE on training set\n",
        "    rmse_train[k] = RMSE(y, ypred_train) ## <--- EDITED LINE\n",
        "            \n",
        "    # feature matrix for test inputs\n",
        "    Phi_test = poly_features(Xtest, k) ## <--- EDITED LINE\n",
        "    \n",
        "    # prediction (TEST SET)\n",
        "    ypred_test = Phi_test @ theta_ml ## <--- EDITED LINE\n",
        "    \n",
        "    # RMSE on test set\n",
        "    rmse_test[k] = RMSE(ytest, ypred_test) ## <--- EDITED LINE\n",
        "    \n",
        "\n",
        "plt.figure()\n",
        "plt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\n",
        "plt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\n",
        "plt.xlabel(\"degree of polynomial\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend([\"training set\", \"test set\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90VRYc0r1fYC"
      },
      "source": [
        "#### Questions:\n",
        "1. What do you observe now?\n",
        "2. Why does the RMSE for the test set not always go down?\n",
        "3. Which polynomial degree would you choose now?\n",
        "4. Plot the fit for the \"best\" polynomial degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "TU68EdjF1fYD",
        "outputId": "e3bb77d4-33bb-461d-f3fb-909b5133713f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# WRITE THE PLOTTING CODE HERE\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "k = 5\n",
        "# feature matrix\n",
        "Phi = poly_features(X, k)\n",
        "\n",
        "# maximum likelihood estimate\n",
        "theta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n",
        "\n",
        "# feature matrix for test inputs\n",
        "Phi_test = poly_features(Xtest, k)\n",
        "\n",
        "ypred_test = Phi_test @ theta_ml ## <--- EDITED LINE (hint: you may require a few lines to do the computation)\n",
        "\n",
        "plt.plot(Xtest, ypred_test) \n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.legend([\"data\", \"maximum likelihood fit\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1C52EY1fYD"
      },
      "source": [
        "#### Question\n",
        "If you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_RMDYNz1fYD"
      },
      "source": [
        "## 2. Maximum A Posteriori Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwBkRBIM1fYD"
      },
      "source": [
        "We are still considering the model\n",
        "$$\n",
        "y = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n",
        "$$\n",
        "We assume that the noise variance $\\sigma^2$ is known."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gf8ST-p1fYD"
      },
      "source": [
        "Instead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters $\\boldsymbol\\theta$, which is given as\n",
        "$$\n",
        "p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n",
        "$$\n",
        "The purpose of the parameter prior $p(\\boldsymbol\\theta)$ is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a \"reasonable\" range of parameter values. Typically, we choose a Gaussian prior $\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)$, centered at $\\boldsymbol 0$ with variance $\\alpha^2$ along each parameter dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjlGNJ611fYD"
      },
      "source": [
        "The MAP estimate of the parameters is\n",
        "$$\n",
        "\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n",
        "$$\n",
        "where $\\sigma^2$ is the variance of the noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qXq811Fd1fYD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDITed FUNCTION\n",
        "def map_estimate_poly(Phi, y, sigma, alpha):\n",
        "    # Phi: training inputs, Size of N x D\n",
        "    # y: training targets, Size of D x 1\n",
        "    # sigma: standard deviation of the noise \n",
        "    # alpha: standard deviation of the prior on the parameters\n",
        "    # returns: MAP estimate theta_map, Size of D x 1\n",
        "    \n",
        "    D = Phi.shape[1] \n",
        "\n",
        "    # SOLUTION\n",
        "    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n",
        "    # theta_map = scipy.linalg.solve(PP, Phi.T @ y) ## using scipy.linalg.solve to solve the linear system\n",
        "    theta_map = scipy.linalg.solve(PP, Phi.T @ y) ## <--- EDITED LINE\n",
        "\n",
        "    return theta_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938Rqspv-tiZ"
      },
      "source": [
        "# **Explanation:**\n",
        "\n",
        "*   The function takes in the inputs Phi (a N x D matrix \n",
        "of training inputs), y (a N x 1 vector of training targets), sigma (the standard deviation of the noise), and alpha (the standard deviation of the prior on the parameters).\n",
        "*   The MAP estimate of the parameters is given by the formula theta_map = (sigma^2/alpha^2 * I + Phi^T Phi)^-1 Phi^T y, where I is the identity matrix. This formula can be derived from Bayes' theorem and assuming a Gaussian prior on the parameters with zero mean and covariance matrix (alpha^2) I.\n",
        "*   The edited function uses this formula to compute the MAP estimate of the parameters, and returns it as the output of the function. It uses scipy.linalg.solve function to solve the linear system Ptheta = q, where P = Phi^T Phi + (sigma/alpha)^2 I and q = Phi^T y.\n",
        "*   The scipy.linalg.solve function takes the matrix P and the vector q as inputs and returns the solution to the linear system, which is the MAP estimate of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BuiilKPB1fYD"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# define the function we wish to estimate later\n",
        "def g(x, sigma):\n",
        "    p = np.hstack([x**0, x**1, np.sin(x)])\n",
        "    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n",
        "    return p @ w + sigma*np.random.normal(size=x.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "collapsed": true,
        "id": "EUy3xsld1fYD",
        "outputId": "41e4abf5-e2c0-465b-e41c-3ae221cbfc7e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Generate some data\n",
        "sigma = 1.0 # noise standard deviation\n",
        "alpha = 1.0 # standard deviation of the parameter prior\n",
        "N = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\n",
        "y = g(X, sigma) # training targets\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "vDFzg5GI1fYD",
        "outputId": "3bb12beb-7324-40cd-a30c-c7a424a45474"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# get the MAP estimate\n",
        "K = 8 # polynomial degree   \n",
        "\n",
        "\n",
        "# feature matrix\n",
        "Phi = poly_features(X, K)\n",
        "\n",
        "theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n",
        "\n",
        "# maximum likelihood estimate\n",
        "theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n",
        "\n",
        "Xtest = np.linspace(-5,5,100).reshape(-1,1)\n",
        "ytest = g(Xtest, sigma)\n",
        "\n",
        "Phi_test = poly_features(Xtest, K)\n",
        "y_pred_map = Phi_test @ theta_map\n",
        "\n",
        "y_pred_mle = Phi_test @ theta_ml\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X, y, '+')\n",
        "plt.plot(Xtest, y_pred_map)\n",
        "plt.plot(Xtest, g(Xtest, 0))\n",
        "plt.plot(Xtest, y_pred_mle)\n",
        "\n",
        "plt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLck8n9CfI81"
      },
      "source": [
        "# Question:\n",
        "1. What is meant by the ground-truth function below input line 27? \n",
        "\n",
        "# Answer:\n",
        "\n",
        "Ground truth function is the main function where we produce our actual data. What we're trying to do is find the function that most closely resembles the ground truth function with map prediction and maximum likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fkZtg7671fYD",
        "outputId": "8409f1de-12a6-43a5-9db6-d5ddebec6d54"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "print(np.hstack([theta_ml, theta_map]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2sBSOhB1fYE"
      },
      "source": [
        "Now, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "collapsed": true,
        "id": "Ffp0s4Zl1fYE",
        "outputId": "e5b7d5cb-994d-4f08-93e4-a80e37829f93"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "K_max = 12 # this is the maximum degree of polynomial we will consider\n",
        "assert(K_max < N) # this is the latest point when we'll run into numerical problems\n",
        "\n",
        "rmse_mle = np.zeros((K_max+1,))\n",
        "rmse_map = np.zeros((K_max+1,))\n",
        "\n",
        "for k in range(K_max+1):\n",
        "   \n",
        "    # feature matrix\n",
        "    Phi = poly_features(X, k)\n",
        "    \n",
        "    # maximum likelihood estimate\n",
        "    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n",
        "    \n",
        "    # predict the function values at the test input locations (maximum likelihood)\n",
        "    y_pred_test = 0*Xtest ## <--- EDIT THIS LINE\n",
        "      \n",
        "    ####################### SOLUTION\n",
        "    # feature matrix for test inputs\n",
        "    Phi_test = poly_features(Xtest, k)\n",
        "    \n",
        "    # prediction\n",
        "    ypred_test_mle = Phi_test @ theta_ml\n",
        "    #######################\n",
        "    \n",
        "    # RMSE on test set (maximum likelihood)\n",
        "    rmse_mle[k] = RMSE(ytest, ypred_test_mle) ## Compute the maximum likelihood estimator, compute the test-set predicitons, compute the RMSE\n",
        "    \n",
        "    # MAP estimate\n",
        "    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n",
        "\n",
        "    # Feature matrix\n",
        "    Phi_test = poly_features(Xtest, k)\n",
        "    \n",
        "    # predict the function values at the test input locations (MAP)\n",
        "    ypred_test_map = Phi_test @ theta_map\n",
        "    \n",
        "    # RMSE on test set (MAP)\n",
        "    rmse_map[k] = RMSE(ytest, ypred_test_map) ## Compute the MAP estimator, compute the test-set predicitons, compute the RMSE\n",
        "\n",
        "plt.figure()\n",
        "plt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\n",
        "plt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\n",
        "plt.xlabel(\"degree of polynomial\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend([\"Maximum likelihood\", \"MAP\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr3kGEQ31fYE"
      },
      "source": [
        "#### Questions:\n",
        "1. What do you observe? \n",
        "  *   Answer: The plot shows the RMSE for the maximum likelihood estimator and the MAP estimator as a function of the degree of polynomial used in the model. We observe that for low degrees of polynomial, the MAP estimator outperforms the maximum likelihood estimator. As the degree of polynomial increases, the RMSE for the MAP estimator increases significantly, while the RMSE for the maximum likelihood estimator increases much more slowly. This is because the MAP estimator incorporates a prior on the parameters, which helps to prevent overfitting.\n",
        "\n",
        "\n",
        "2. What is the influence of the prior variance on the parameters ($\\alpha^2$)? Change the parameter and describe what happens.\n",
        "  *   Answer: \n",
        "    *   The parameter ($\\alpha^2$) controls the strength of the prior distribution on the model parameters. Increasing ($\\alpha^2$) makes the prior distribution more spread out and less concentrated around zero, while decreasing ($\\alpha^2$) makes the prior distribution more concentrated around zero. \n",
        "    *   If ($\\alpha^2$) is small, the prior distribution will be narrow and peaked around zero, meaning that the MAP estimate of the parameters will be strongly influenced by the data. This can lead to overfitting, especially if the amount of training data is small. In contrast, if ($\\alpha^2$) is large, the prior distribution will be more spread out, meaning that the MAP estimate will be less sensitive to the data, which can help prevent overfitting.\n",
        "    *   In general, the choice of ($\\alpha^2$) should be based on prior knowledge about the distribution of the parameters. If there is little prior knowledge, a relatively large value of ($\\alpha^2$) may be appropriate to provide a weakly informative prior that helps prevent overfitting. If there is more prior knowledge, a smaller value of ($\\alpha^2$) may be appropriate to provide a more informative prior.\n",
        "    *   As a summary, Increasing alpha is increasing the uncertainty of our bias. As alpha increases, the standard deviation of our prior distribution increases. The lower the alpha, the greater the impact of our prediction on the outcome. As we increased the alpha, we got closer to the MLE solution because there is no such term in the MLE formula. ie there will be no regularization, like reducing regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsqQCyMa1fYE"
      },
      "source": [
        "## 3. Bayesian Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S6z1u3RV1fYE"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Test inputs\n",
        "Ntest = 200\n",
        "Xtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n",
        "\n",
        "prior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\n",
        "noise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n",
        "\n",
        "pol_deg = 3 # degree of the polynomial we consider at the moment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adPznYk91fYE"
      },
      "source": [
        "Assume a parameter prior $p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)$. For every test input $\\boldsymbol x_*$ we obtain the \n",
        "prior mean\n",
        "$$\n",
        "E[f(\\boldsymbol x_*)] = 0\n",
        "$$\n",
        "and the prior (marginal) variance (ignoring the noise contribution)\n",
        "$$\n",
        "V[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n",
        "$$\n",
        "where $\\boldsymbol\\phi(\\cdot)$ is the feature map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "collapsed": true,
        "id": "awo4VYBH1fYF",
        "outputId": "c5deba26-bebc-4b03-9c95-620d889e8ec0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDITED CELL\n",
        "\n",
        "# compute the feature matrix for the test inputs\n",
        "Phi_test = np.zeros((Ntest, pol_deg+1))  # N x (pol_deg+1) feature matrix <--- EDIT THIS LINE\n",
        "raise NotImplementedError\n",
        "\n",
        "# compute the (marginal) prior at the test input locations\n",
        "# prior mean\n",
        "# prior_mean = np.ones((Ntest,1))  # prior mean at test inputs (size: (Ntest,1)) <-- EDIT THIS LINE\n",
        "# raise NotImplementedError\n",
        "prior_mean = np.zeros((Ntest,1)) # prior mean <-- SOLUTION\n",
        "\n",
        "# prior variance\n",
        "\"\"\"\n",
        "full_covariance = np.zeros((Ntest, Ntest)) # N x N covariance matrix of all function values <-- EDIT THIS LINE\n",
        "prior_marginal_var = 0 # marginal of size (N, )\n",
        "raise NotImplementedError\n",
        "\"\"\"\n",
        "full_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\n",
        "prior_marginal_var =  np.diag(full_covariance)\n",
        "\n",
        "# Let us visualize the prior over functions\n",
        "plt.figure()\n",
        "plt.plot(Xtest, prior_mean, color=\"k\")\n",
        "\n",
        "conf_bound1 = np.sqrt(prior_marginal_var).flatten()\n",
        "conf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\n",
        "conf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\n",
        "plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n",
        "             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\n",
        "plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n",
        "                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\n",
        "plt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n",
        "                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n",
        "\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.title(\"Prior over functions\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ewGYbRw1fYF"
      },
      "source": [
        "Now, we will use this prior distribution and sample functions from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DR_6sxpi1fYF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# samples from the prior\n",
        "num_samples = 10\n",
        "\n",
        "# We first need to generate random weights theta_i, which we sample from the parameter prior\n",
        "random_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n",
        "\n",
        "# Now, we compute the induced random functions, evaluated at the test input locations\n",
        "# Every function sample is given as f_i = Phi * theta_i, \n",
        "# where theta_i is a sample from the parameter prior\n",
        "\n",
        "sample_function = np.zeros((Ntest,)) # <-- EDIT THIS LINE\n",
        "raise NotImplementedError\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Xtest, sample_function, color=\"r\")\n",
        "plt.title(\"Plausible functions under the prior\")\n",
        "print(\"Every sampled function is a polynomial of degree \"+str(pol_deg));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BHmtLs81fYF"
      },
      "source": [
        "Now we are given some training inputs $\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N$, which we collect in a matrix $\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WRXYCOqU1fYF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "N = 10\n",
        "X = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\n",
        "y = g(X, np.sqrt(noise_var)) # training targets, size Nx1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3SzyP8C1fYF"
      },
      "source": [
        "Now, let us compute the posterior "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NMJ4Mpf11fYF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS FUNCTION\n",
        "\n",
        "def polyfit(X, y, K, prior_var, noise_var):\n",
        "    # X: training inputs, size N x D\n",
        "    # y: training targets, size N x 1\n",
        "    # K: degree of polynomial we consider\n",
        "    # prior_var: prior variance of the parameter distribution\n",
        "    # sigma: noise variance\n",
        "    \n",
        "    jitter = 1e-08 # increases numerical stability\n",
        "    \n",
        "    Phi = poly_features(X, K) # N x (K+1) feature matrix \n",
        "    \n",
        "    # Compute maximum likelihood estimate\n",
        "    theta_ml = np.zeros((K+1,1)) # <-- EDIT THIS LINE \n",
        "    \n",
        "    # MAP estimate\n",
        "    theta_map = np.zeros((K+1,1)) # <-- EDIT THIS LINE \n",
        "    \n",
        "    # Parameter posterior\n",
        "    SN = np.zeros(K+1) # covariance matrix of the parameter posterior # <-- EDIT THIS LINE \n",
        "    mN = np.zeros((K+1,1)) # mean vector of the parameter posterior   # <-- EDIT THIS LINE \n",
        "    \n",
        "    raise NotImplementedError\n",
        "    \n",
        "    return (theta_ml, theta_map, mN, SN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_9UAvGIY1fYG"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "theta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2OIFqro1fYG"
      },
      "source": [
        "Now, let's make predictions (ignoring the measurement noise). We obtain three predictors:\n",
        "\\begin{align}\n",
        "&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n",
        "&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n",
        "&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n",
        "\\end{align}\n",
        "We already computed all quantities. Write some code that implements all three predictors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QAtb4bBE1fYG"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# predictions (ignoring the measurement/observations noise)\n",
        "\n",
        "# maximum likelihood predictions (just the mean)\n",
        "m_mle_test = np.zeros((Ntest,1)) # <-- EDIT THIS LINE\n",
        "\n",
        "# MAP predictions (just the mean)\n",
        "m_map_test = np.zeros((Ntest,1)) # <-- EDIT THIS LINE\n",
        "\n",
        "# predictive distribution (Bayesian linear regression)\n",
        "# mean prediction\n",
        "mean_blr = np.zeros((Ntest,1)) # <-- EDIT THIS LINE\n",
        "# variance prediction\n",
        "cov_blr =  np.ones((Ntest,Ntest)) # <-- EDIT THIS LINE\n",
        "\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VPE3au1p1fYG"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÇekirdek başlatılamadı. \n",
            "\u001b[1;31mUnable to start Kernel 'work_env (Python 3.10.10)' due to connection timeout. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# plot the posterior\n",
        "plt.figure()\n",
        "plt.plot(X, y, \"+\")\n",
        "plt.plot(Xtest, m_mle_test)\n",
        "plt.plot(Xtest, m_map_test)\n",
        "var_blr = np.diag(cov_blr)\n",
        "conf_bound1 = np.sqrt(var_blr).flatten()\n",
        "conf_bound2 = 2.0*np.sqrt(var_blr).flatten()\n",
        "conf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n",
        "\n",
        "plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n",
        "                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\n",
        "plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n",
        "                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\n",
        "plt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n",
        "                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n",
        "plt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\n",
        "plt.xlabel('$x$');\n",
        "plt.ylabel('$y$');"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
